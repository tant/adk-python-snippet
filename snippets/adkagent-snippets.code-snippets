{
  "ADKAgentSnippet": {
    "prefix": "adkagent",
    "body": [
      "# 1. Imports",
      "# Import necessary components from ADK and other libraries",
      "from adk.agent import LlmAgent, BaseAgent # LlmAgent is common [4], BaseAgent is base class [5, 6]",
      "from adk.tools import Tool, ToolContext # Define custom tools with Tool, ToolContext for state [7, 8]",
      "from adk.agent_types import SequentialAgent, ParallelAgent, LoopAgent, CustomAgent # For multi-agent teams [5, 9]",
      "from adk.callback import CallbackContext, LlmRequest, LlmResponse # For callbacks [10, 11]",
      "from adk.models import LiteLlm # For multi-model support [12-14]",
      "",
      "from typing import Dict, Any # Useful for type hints in tools and callbacks",
      "import os # Often needed to access environment variables for API keys",
      "",
      "# (Optional) Import constants for models if defined elsewhere (e.g., in a config file)",
      "# from .config import MODEL_GEMINI_FLASH, MODEL_GPT_4O # Example",
      "",
      "# (Optional) Import other tools if using built-in or third-party ones",
      "# from adk.tools.google import SearchTool # Example built-in tool [15]",
      "# from adk.tools.langchain import LangChainTool # Example third-party integration [15]",
      "",
      "# 2. Global Configuration (Optional but Recommended)",
      "# Define model names or other configuration settings using constants",
      "# Ensure API keys are set up in your environment or a .env file [14, 16]",
      "# Example using LiteLlm to specify models from different providers [13, 14]",
      "# Make sure to install litellm: pip install litellm [16]",
      "PRIMARY_LLM = LiteLlm(model=\"gemini/gemini-1.5-flash-latest\") # Example Gemini model",
      "SECONDARY_LLM = LiteLlm(model=\"openai/gpt-4o\") # Example OpenAI model [13, 14, 17]",
      "TERTIARY_LLM = LiteLlm(model=\"anthropic/claude-3-5-sonnet-20240620\") # Example Claude model [13, 14, 17]",
      "",
      "# 3. Tool Definitions",
      "# Define the functions that your agent(s) will use as tools [18, 19]",
      "# Tools are regular Python functions [18]",
      "# They must have clear and descriptive docstrings [20, 21]",
      "# Docstrings tell the LLM *what* the tool does, *when* to use it, and *what arguments* it needs [20]",
      "",
      "def example_tool_function(argument_name: str) -> Dict[str, Any]:",
      "    \"\"\"",
      "    A brief description of what this tool does.",
      "    Example: Fetches information based on the provided argument.",
      "",
      "    Args:",
      "        argument_name: A description of the argument (e.g., The query string for the search).",
      "",
      "    Returns:",
      "        A dictionary containing the result (e.g., {\"status\": \"success\", \"data\": \"some result\"}) [22].",
      "        Or an error format if something goes wrong (e.g., {\"status\": \"error\", \"error_message\": \"Details of the error\"}) [22].",
      "    \"\"\"",
      "    print(f\"--- Tool: example_tool_function called with argument_name='{argument_name}' ---\") # Good for debugging [21]",
      "    try:",
      "        # --- Tool Logic Goes Here ---",
      "        # Example: Simulate fetching data or performing an action",
      "        if argument_name == \"test\":",
      "            result_data = \"This is a successful test result.\"",
      "            return {\"status\": \"success\", \"data\": result_data}",
      "        else:",
      "            # Simulate an error for other inputs",
      "            return {\"status\": \"error\", \"error_message\": f\"Input '{argument_name}' not supported by example_tool_function.\"}",
      "",
      "    except Exception as e:",
      "        # Implement comprehensive error handling [22]",
      "        print(f\"Error in example_tool_function: {e}\")",
      "        return {\"status\": \"error\", \"error_message\": f\"An error occurred while using example_tool_function: {e}\"}",
      "",
      "# Example of a state-aware tool that accepts ToolContext [7, 8]",
      "# ToolContext allows access to session state [7, 8]",
      "def stateful_tool_function(city: str, tool_context: ToolContext) -> Dict[str, Any]:",
      "    \"\"\"",
      "    Gets the weather for a city, using user preference from session state.",
      "",
      "    Args:",
      "        city: The name of the city (e.g., \"London\").",
      "        tool_context: Provides access to the session state. ADK injects this automatically [8].",
      "",
      "    Returns:",
      "        A dictionary containing the weather report or an error.",
      "    \"\"\"",
      "    print(f\"--- Tool: stateful_tool_function called for city='{city}' ---\")",
      "    try:",
      "        # Read from session state [7, 8]",
      "        # Use .get() with a default value for safety [8]",
      "        temp_unit = tool_context.state.get('user_preference_temperature_unit', 'Celsius')",
      "        print(f\"--- Reading state: user_preference_temperature_unit = {temp_unit} ---\")",
      "",
      "        # --- Tool Logic Goes Here ---",
      "        # Simulate fetching weather data and applying preference",
      "        if city == \"London\":",
      "            temp_celsius = 15",
      "            if temp_unit.lower() == 'fahrenheit':",
      "                temp_value = (temp_celsius * 9/5) + 32",
      "                unit = 'Fahrenheit'",
      "            else: # Default to Celsius",
      "                temp_value = temp_celsius",
      "                unit = 'Celsius'",
      "            report = f\"The weather in {city} is sunny with a temperature of {temp_value:.1f}°{unit}.\"",
      "",
      "        elif city == \"New York\":",
      "            temp_celsius = 25",
      "            if temp_unit.lower() == 'fahrenheit':",
      "                temp_value = (temp_celsius * 9/5) + 32",
      "                unit = 'Fahrenheit'",
      "            else: # Default to Celsius",
      "                temp_value = temp_celsius",
      "                unit = 'Celsius'",
      "            report = f\"The weather in {city} is cloudy with a temperature of {temp_value:.1f}°{unit}.\"",
      "        elif city == \"Paris\":",
      "            # Example: Simulate a policy restriction or error based on city [23, 24]",
      "            # Note: A before_tool_callback is a better place for policy enforcement [23, 25]",
      "            # This is just for demonstrating tool logic based on city",
      "            return {\"status\": \"error\", \"error_message\": \"Sorry, checking weather for Paris is currently restricted.\"}",
      "        else:",
      "            return {\"status\": \"error\", \"error_message\": f\"Unknown city: {city}\"}",
      "",
      "        # (Optional) Write to session state [7]",
      "        tool_context.state['last_city_checked_stateful'] = city",
      "        print(f\"--- Writing state: last_city_checked_stateful = {city} ---\")",
      "",
      "        return {\"status\": \"success\", \"report\": report}",
      "",
      "    except Exception as e:",
      "        print(f\"Error in stateful_tool_function: {e}\")",
      "        return {\"status\": \"error\", \"error_message\": f\"An error occurred: {e}\"}",
      "",
      "# 4. Callback Definitions (Optional)",
      "# Define functions for safety guardrails or modifying requests/results [10, 11]",
      "# These are added to the agent configuration [26]",
      "",
      "def block_keyword_guardrail(callback_context: CallbackContext, llm_request: LlmRequest) -> LlmResponse | None:",
      "    \"\"\"",
      "    Example before_model_callback to block requests containing a specific keyword.",
      "    Runs BEFORE calling the LLM [10].",
      "",
      "    Args:",
      "        callback_context: Context about the agent and session [10].",
      "        llm_request: The request payload about to be sent to the LLM [10].",
      "",
      "    Returns:",
      "        An LlmResponse to block the call, or None to allow it [10].",
      "    \"\"\"",
      "    user_message = llm_request.contents[-1].parts.text # Access the last user message [27]",
      "    keyword = \"BLOCK\" # Example keyword [27]",
      "    print(f\"--- Running before_model_callback. Checking for keyword '{keyword}' in: '{user_message}' ---\")",
      "",
      "    if keyword.lower() in user_message.lower():",
      "        print(f\"--- Blocking LLM call! Keyword '{keyword}' found. ---\")",
      "        # You can also update state here if needed [27]",
      "        callback_context.state['blocked_last_turn'] = True",
      "        # Return an LlmResponse to block the LLM call [10]",
      "        return LlmResponse(contents=[{\"text\": f\"Your request contains a prohibited keyword ('{keyword}') and has been blocked.\"}])",
      "    else:",
      "        print(\"--- Keyword not found. Allowing LLM call. ---\")",
      "        callback_context.state['blocked_last_turn'] = False",
      "        return None # Return None to allow the LLM call to proceed [10]",
      "",
      "def tool_argument_guardrail(tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext) -> Dict[str, Any] | None:",
      "    \"\"\"",
      "    Example before_tool_callback to block tool execution based on arguments.",
      "    Runs BEFORE executing the tool function [11].",
      "",
      "    Args:",
      "        tool: The tool object about to be called [11].",
      "        args: The arguments the LLM generated for the tool [11].",
      "        tool_context: Context about the agent and session [11].",
      "",
      "    Returns:",
      "        A dictionary to override the tool result (blocking the actual call), or None to allow the tool call [11].",
      "    \"\"\"",
      "    print(f\"--- Running before_tool_callback for tool '{tool.name}' with args: {args} ---\")",
      "    if tool.name == \"stateful_tool_function\" and args.get('city') == 'Paris':",
      "        print(\"--- Blocking tool execution for Paris! Policy restriction. ---\")",
      "        tool_context.state['blocked_tool_paris'] = True",
      "        # Return a dictionary to simulate the tool's result and block the actual call [11]",
      "        return {\"status\": \"error\", \"error_message\": \"Policy restriction: Checking weather for Paris is not allowed.\"}",
      "    else:",
      "        print(\"--- Allowing tool execution. ---\")",
      "        tool_context.state['blocked_tool_paris'] = False",
      "        return None # Return None to allow the tool function to execute [11]",
      "",
      "# 5. Sub-Agent Definitions (For Multi-Agent Teams)",
      "# If building a multi-agent team, define specialist sub-agents here first [28-31]",
      "# Give them clear instructions [30] and crucially, clear descriptions [30, 32]",
      "# The root agent uses the descriptions to decide when to delegate [30, 32, 33]",
      "",
      "greeting_agent = LlmAgent(",
      "    name=\"greeting_agent\",",
      "    model=PRIMARY_LLM, # Can use a different model for sub-agents [28]",
      "    description=\"Handles simple greetings and welcoming messages.\", # CRUCIAL for delegation [30, 32]",
      "    instruction=\"Your ONLY task is to respond to greeting messages like 'hello', 'hi', 'hey', etc. Respond with a friendly greeting. DO NOT attempt to answer any other questions or use any tools.\" # [30]",
      "    # Sub-agents can also have tools, output_key, callbacks, etc.",
      "    # tools=[say_hello_tool], # Assuming a say_hello_tool was defined above",
      ")",
      "",
      "farewell_agent = LlmAgent(",
      "    name=\"farewell_agent\",",
      "    model=PRIMARY_LLM, # Can use a different model",
      "    description=\"Handles farewell messages and closing remarks.\", # CRUCIAL for delegation [30, 32]",
      "    instruction=\"Your ONLY task is to respond to farewell messages like 'bye', 'goodbye', 'see you later', etc. Respond with a polite farewell. DO NOT attempt to answer any other questions or use any tools.\" # [30]",
      "    # tools=[say_goodbye_tool], # Assuming a say_goodbye_tool was defined above",
      ")",
      "",
      "# 6. Root/Main Agent Definition",
      "# Define the agent instance that you want to run [4, 34]",
      "# This is often the orchestrator (root agent) for multi-agent teams [28, 31, 33]",
      "# Configure it with its model, instructions, tools, and potentially sub-agents, callbacks, etc.",
      "",
      "# Example of a single agent (like the initial weather bot in the tutorial)",
      "# weather_agent_single = LlmAgent(",
      "#     name=\"weather_agent\", # A unique identifier [34]",
      "#     model=PRIMARY_LLM, # The LLM it uses [34] (or a LiteLlm instance [13, 14])",
      "#     description=\"An agent that can provide weather information.\", # Describes its purpose [34]",
      "#     instruction=\"You are a helpful weather assistant. Your goal is to provide accurate weather information to the user using the available tools. If the user asks about the weather, use the get_weather tool. If you cannot find the weather, inform the user.\" # [34, 35]",
      "#     tools=[stateful_tool_function], # A list of tool functions/instances this agent can use [34]",
      "#     output_key=\"last_weather_report\", # Automatically save the final response to this state key [7, 16]",
      "#     # Add callbacks if desired [26]",
      "#     # before_model_callback=[block_keyword_guardrail],",
      "#     # before_tool_callback=[tool_argument_guardrail],",
      "# )",
      "",
      "# Example of a root agent for a multi-agent team [31, 33]",
      "# This agent includes the sub-agents and instructions on when to delegate [33, 36]",
      "root_agent = LlmAgent(",
      "    name=\"weather_agent_team\", # A unique identifier [34]",
      "    model=PRIMARY_LLM, # The LLM it uses [34] (or a LiteLlm instance [13, 14])",
      "    description=\"A versatile conversational agent that provides weather information and handles greetings and farewells.\", # Describes its purpose [34, 35]",
      "    instruction=(",
      "        \"You are a friendly conversational AI agent. Your primary function is to help users with weather queries using your tools. \"",
      "        \"You also have specialized sub-agents for handling greetings and farewells. \"",
      "        \"**Crucially, you must delegate** simple greetings (like 'hello', 'hi') to the 'greeting_agent' and simple farewells (like 'bye', 'goodbye') to the 'farewell_agent'. \"",
      "        \"For all other queries, especially those asking for weather information, handle them yourself using your available tools. \"",
      "        \"Use the 'stateful_tool_function' tool when the user asks for weather in a specific city. \"",
      "        \"Be polite and helpful in your responses.\"",
      "    ),",
      "    tools=[stateful_tool_function], # Tools available to the root agent itself [34]",
      "    sub_agents=[greeting_agent, farewell_agent], # List of sub-agent instances for automatic delegation (Auto-Flow) [33, 37]",
      "    output_key=\"last_agent_response\", # Automatically save the root agent's final response [7, 16]",
      "    before_model_callback=[block_keyword_guardrail], # Add callbacks if desired [26]",
      "    before_tool_callback=[tool_argument_guardrail],",
      "    # Note: Callbacks on the root agent do NOT automatically apply to sub-agents [38]",
      ")",
      "",
      "# 7. Define the Agent Variable",
      "# This is the final variable that ADK's built-in tools (adk run, adk web)",
      "# will look for and use as the entry point for your agent or team [39]",
      "# Ensure this variable is set to the agent instance you want to expose",
      "# Example: agent = weather_agent_single # For a single agent",
      "agent = root_agent # For the multi-agent team",
      "",
      "# 8. (Optional) If you need to run this script standalone (outside adk run/web)",
      "# you would typically set up a Runner and SessionService here [40]",
      "# See previous explanations and tutorial steps [40-43]",
      "# from adk.runner import Runner",
      "# from adk.session import InMemorySessionService, Session",
      "# import asyncio",
      "#",
      "# async def run_conversation(user_input: str):",
      "#     session_service = InMemorySessionService() # Or another implementation",
      "#     user_id = \"test_user\"",
      "#     session_id = \"test_session\"",
      "#     # Initialize session state if needed for the first turn [44]",
      "#     session_service.get_session(user_id, session_id).state['user_preference_temperature_unit'] = 'Fahrenheit'",
      "#",
      "#     runner = Runner(agent=agent, session_service=session_service) # Configure the runner [40]",
      "#",
      "#     print(f\"\\n--- User: {user_input} ---\")",
      "#     # Iterate through events yielded by the runner [40]",
      "#     async for event in runner.run_async(user_id, session_id, user_input):",
      "#         # Process events (e.g., print intermediate thoughts, tool calls, etc.)",
      "#         # Check if the event is the final response [40]",
      "#         if event.is_final_response():",
      "#             print(f\"--- Agent: {event.response.text} ---\")",
      "#",
      "#     # (Optional) Inspect final state after the turn [43]",
      "#     final_session = session_service.get_session(user_id, session_id)",
      "#     print(f\"--- Final Session State: {final_session.state} ---\")",
      "#",
      "# # Example usage (only if running this file directly)",
      "# if __name__ == \"__main__\":",
      "#     # asyncio.run(run_conversation(\"Hello there!\"))",
      "#     # asyncio.run(run_conversation(\"What is the weather in London?\"))",
      "#     # asyncio.run(run_conversation(\"What is the weather in New York?\"))",
      "#     # asyncio.run(run_conversation(\"Tell me about BLOCK\")) # Test model guardrail",
      "#     # asyncio.run(run_conversation(\"What's the weather in Paris?\")) # Test tool guardrail",
      "#     # asyncio.run(run_conversation(\"Thanks, bye!\"))",
      "#     pass # Often commented out when using adk run/web"
    ],
    "description": "Inserts the ADK Agent template."
  }
}